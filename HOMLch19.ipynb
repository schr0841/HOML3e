{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 19 – Training and Deploying TensorFlow Models at Scale\n",
    "\n",
    "This notebook contains all the sample code and solutions to the exercises in chapter 19.\n",
    "\n",
    "## Setup\n",
    "This project requires Python 3.7 or above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "assert sys.version_info >= (3, 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Warning**: the latest TensorFlow versions are based on Keras 3. For chapters 10-15, it wasn't too hard to update the code to support Keras 3, but unfortunately it's much harder for this chapter, so I've had to revert to Keras 2. To do that, I set the ```TF_USE_LEGACY_KERAS ```environment variable to ```\"1\" ```and import the ```tf_keras``` package. This ensures that ```tf.keras``` points to ```tf_keras```, which is Keras 2.*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IS_COLAB = \"google.colab\" in sys.modules\n",
    "if IS_COLAB:\n",
    "    import os\n",
    "    os.environ[\"TF_USE_LEGACY_KERAS\"] = \"1\"\n",
    "    import tf_keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And TensorFlow ≥ 2.8:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from packaging import version\n",
    "import tensorflow as tf\n",
    "\n",
    "assert version.parse(tf.__version__) >= version.parse(\"2.8.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If running on Colab or Kaggle, you need to install the Google AI Platform client library, which will be used later in this notebook. You can ignore the warnings about version incompatibilities.\n",
    "\n",
    "* **Warning**: On Colab, you must restart the Runtime after the installation, and continue with the next cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if \"google.colab\" in sys.modules or \"kaggle_secrets\" in sys.modules:\n",
    "    %pip install -q -U google-cloud-aiplatform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This chapter discusses how to run or train a model on one or more GPUs, so let's make sure there's at least one, or else issue a warning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not tf.config.list_physical_devices('GPU'):\n",
    "    print(\"No GPU was detected. Neural nets can be very slow without a GPU.\")\n",
    "    if \"google.colab\" in sys.modules:\n",
    "        print(\"Go to Runtime > Change runtime and select a GPU hardware \"\n",
    "              \"accelerator.\")\n",
    "    if \"kaggle_secrets\" in sys.modules:\n",
    "        print(\"Go to Settings > Accelerator and select GPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serving a TensorFlow Model\n",
    "Let's start by deploying a model using TF Serving, then we'll deploy to Google Vertex AI.\n",
    "\n",
    "## Using TensorFlow Serving\n",
    "The first thing we need to do is to build and train a model, and export it to the SavedModel format.\n",
    "\n",
    "## Exporting SavedModels\n",
    "Let's load the MNIST dataset, scale it, and split it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1719/1719 [==============================] - 3s 1ms/step - loss: 0.6945 - accuracy: 0.8221 - val_loss: 0.3683 - val_accuracy: 0.9008\n",
      "Epoch 2/10\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.3535 - accuracy: 0.9019 - val_loss: 0.2979 - val_accuracy: 0.9166\n",
      "Epoch 3/10\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.3037 - accuracy: 0.9143 - val_loss: 0.2642 - val_accuracy: 0.9266\n",
      "Epoch 4/10\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2733 - accuracy: 0.9229 - val_loss: 0.2420 - val_accuracy: 0.9342\n",
      "Epoch 5/10\n",
      "1719/1719 [==============================] - 3s 1ms/step - loss: 0.2503 - accuracy: 0.9295 - val_loss: 0.2239 - val_accuracy: 0.9376\n",
      "Epoch 6/10\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2318 - accuracy: 0.9350 - val_loss: 0.2104 - val_accuracy: 0.9424\n",
      "Epoch 7/10\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2164 - accuracy: 0.9389 - val_loss: 0.1965 - val_accuracy: 0.9464\n",
      "Epoch 8/10\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2032 - accuracy: 0.9427 - val_loss: 0.1880 - val_accuracy: 0.9500\n",
      "Epoch 9/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.1917 - accuracy: 0.9458 - val_loss: 0.1780 - val_accuracy: 0.9518\n",
      "Epoch 10/10\n",
      "1719/1719 [==============================] - 3s 1ms/step - loss: 0.1815 - accuracy: 0.9486 - val_loss: 0.1691 - val_accuracy: 0.9530\n",
      "INFO:tensorflow:Assets written to: my_mnist_model\\0001\\assets\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "\n",
    "# extra code – load and split the MNIST dataset\n",
    "mnist = tf.keras.datasets.mnist.load_data()\n",
    "(X_train_full, y_train_full), (X_test, y_test) = mnist\n",
    "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "\n",
    "# extra code – build & train an MNIST model (also handles image preprocessing)\n",
    "tf.random.set_seed(42)\n",
    "tf.keras.backend.clear_session()\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=[28, 28], dtype=tf.uint8),\n",
    "    tf.keras.layers.Rescaling(scale=1 / 255),\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=tf.keras.optimizers.SGD(learning_rate=1e-2),\n",
    "              metrics=[\"accuracy\"])\n",
    "model.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid))\n",
    "\n",
    "model_name = \"my_mnist_model\"\n",
    "model_version = \"0001\"\n",
    "model_path = Path(model_name) / model_version\n",
    "model.save(model_path, save_format=\"tf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the file tree (we've discussed what each of these file is used for in chapter 10):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#os.chdir(\"/content/drive/My Drive/path/to/your/model\")\n",
    "#os.chdir(\"C:/Users/schre/OneDrive/Documents/GitHub/HOML3e/\")\n",
    "os.chdir(\"C:/Users/schre/OneDrive/Documents/GitHub/HOML3e/my_mnist_model\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\schre\\OneDrive\\Documents\\GitHub\\HOML3e\\my_mnist_model\\0001\n",
      "C:\\Users\\schre\\OneDrive\\Documents\\GitHub\\HOML3e\\my_mnist_model\n",
      "['0001']\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Replace with the actual absolute path to your model directory\n",
    "absolute_path = r\"C:/Users/schre/OneDrive/Documents/GitHub/HOML3e/my_mnist_model/0001/\" # use raw string (r\"\") to avoid escape sequence issues.\n",
    "model_path = Path(absolute_path)\n",
    "\n",
    "print(model_path)\n",
    "print(model_path.parent)\n",
    "print(os.listdir(model_path.parent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\schre\\\\OneDrive\\\\Documents\\\\GitHub\\\\HOML3e\\\\my_mnist_model\\\\0001',\n",
       " 'C:\\\\Users\\\\schre\\\\OneDrive\\\\Documents\\\\GitHub\\\\HOML3e\\\\my_mnist_model\\\\0001\\\\assets',\n",
       " 'C:\\\\Users\\\\schre\\\\OneDrive\\\\Documents\\\\GitHub\\\\HOML3e\\\\my_mnist_model\\\\0001\\\\keras_metadata.pb',\n",
       " 'C:\\\\Users\\\\schre\\\\OneDrive\\\\Documents\\\\GitHub\\\\HOML3e\\\\my_mnist_model\\\\0001\\\\saved_model.pb',\n",
       " 'C:\\\\Users\\\\schre\\\\OneDrive\\\\Documents\\\\GitHub\\\\HOML3e\\\\my_mnist_model\\\\0001\\\\variables',\n",
       " 'C:\\\\Users\\\\schre\\\\OneDrive\\\\Documents\\\\GitHub\\\\HOML3e\\\\my_mnist_model\\\\0001\\\\variables\\\\variables.data-00000-of-00001',\n",
       " 'C:\\\\Users\\\\schre\\\\OneDrive\\\\Documents\\\\GitHub\\\\HOML3e\\\\my_mnist_model\\\\0001\\\\variables\\\\variables.index']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([str(path) for path in model_path.parent.glob(\"**/*\")])  # extra code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('C:/Users/schre/OneDrive/Documents/GitHub/HOML3e/my_mnist_model/0001')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('C:/Users/schre/OneDrive/Documents/GitHub/HOML3e/my_mnist_model')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path.parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['assets', 'keras_metadata.pb', 'saved_model.pb', 'variables']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.listdir(model_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the SavedModel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The given SavedModel contains the following tag-sets:\n",
      "'serve'\n"
     ]
    }
   ],
   "source": [
    "!saved_model_cli show --dir {model_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:\n",
      "\n",
      "signature_def['__saved_model_init_op']:\n",
      "  The given SavedModel SignatureDef contains the following input(s):\n",
      "  The given SavedModel SignatureDef contains the following output(s):\n",
      "    outputs['__saved_model_init_op'] tensor_info:\n",
      "        dtype: DT_INVALID\n",
      "        shape: unknown_rank\n",
      "        name: NoOp\n",
      "  Method name is: \n",
      "\n",
      "signature_def['serving_default']:\n",
      "  The given SavedModel SignatureDef contains the following input(s):\n",
      "    inputs['flatten_input'] tensor_info:\n",
      "        dtype: DT_UINT8\n",
      "        shape: (-1, 28, 28)\n",
      "        name: serving_default_flatten_input:0\n",
      "  The given SavedModel SignatureDef contains the following output(s):\n",
      "    outputs['dense_1'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (-1, 10)\n",
      "        name: StatefulPartitionedCall:0\n",
      "  Method name is: tensorflow/serving/predict\n",
      "\n",
      "Concrete Functions:\n",
      "  Function Name: '__call__'\n",
      "    Option #1\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          inputs: TensorSpec(shape=(None, 28, 28), dtype=tf.uint8, name='inputs')\n",
      "        Argument #2\n",
      "          DType: bool\n",
      "          Value: False\n",
      "        Argument #3\n",
      "          DType: NoneType\n",
      "          Value: None\n",
      "    Option #2\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          flatten_input: TensorSpec(shape=(None, 28, 28), dtype=tf.uint8, name='flatten_input')\n",
      "        Argument #2\n",
      "          DType: bool\n",
      "          Value: False\n",
      "        Argument #3\n",
      "          DType: NoneType\n",
      "          Value: None\n",
      "    Option #3\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          flatten_input: TensorSpec(shape=(None, 28, 28), dtype=tf.uint8, name='flatten_input')\n",
      "        Argument #2\n",
      "          DType: bool\n",
      "          Value: True\n",
      "        Argument #3\n",
      "          DType: NoneType\n",
      "          Value: None\n",
      "    Option #4\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          inputs: TensorSpec(shape=(None, 28, 28), dtype=tf.uint8, name='inputs')\n",
      "        Argument #2\n",
      "          DType: bool\n",
      "          Value: True\n",
      "        Argument #3\n",
      "          DType: NoneType\n",
      "          Value: None\n",
      "\n",
      "  Function Name: '_default_save_signature'\n",
      "    Option #1\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          flatten_input: TensorSpec(shape=(None, 28, 28), dtype=tf.uint8, name='flatten_input')\n",
      "\n",
      "  Function Name: 'call_and_return_all_conditional_losses'\n",
      "    Option #1\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          inputs: TensorSpec(shape=(None, 28, 28), dtype=tf.uint8, name='inputs')\n",
      "        Argument #2\n",
      "          DType: bool\n",
      "          Value: False\n",
      "        Argument #3\n",
      "          DType: NoneType\n",
      "          Value: None\n",
      "    Option #2\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          flatten_input: TensorSpec(shape=(None, 28, 28), dtype=tf.uint8, name='flatten_input')\n",
      "        Argument #2\n",
      "          DType: bool\n",
      "          Value: True\n",
      "        Argument #3\n",
      "          DType: NoneType\n",
      "          Value: None\n",
      "    Option #3\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          flatten_input: TensorSpec(shape=(None, 28, 28), dtype=tf.uint8, name='flatten_input')\n",
      "        Argument #2\n",
      "          DType: bool\n",
      "          Value: False\n",
      "        Argument #3\n",
      "          DType: NoneType\n",
      "          Value: None\n",
      "    Option #4\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          inputs: TensorSpec(shape=(None, 28, 28), dtype=tf.uint8, name='inputs')\n",
      "        Argument #2\n",
      "          DType: bool\n",
      "          Value: True\n",
      "        Argument #3\n",
      "          DType: NoneType\n",
      "          Value: None\n"
     ]
    }
   ],
   "source": [
    "!saved_model_cli show --dir {model_path} --all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The given SavedModel MetaGraphDef contains SignatureDefs with the following keys:\n",
      "SignatureDef key: \"__saved_model_init_op\"\n",
      "SignatureDef key: \"serving_default\"\n"
     ]
    }
   ],
   "source": [
    "!saved_model_cli show --dir {model_path} --tag_set serve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\schre\\anaconda3\\envs\\tf_dev\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\schre\\anaconda3\\envs\\tf_dev\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\schre\\anaconda3\\envs\\tf_dev\\Scripts\\saved_model_cli.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\schre\\anaconda3\\envs\\tf_dev\\lib\\site-packages\\tensorflow\\python\\tools\\saved_model_cli.py\", line 1285, in main\n",
      "    args.func(args)\n",
      "  File \"C:\\Users\\schre\\anaconda3\\envs\\tf_dev\\lib\\site-packages\\tensorflow\\python\\tools\\saved_model_cli.py\", line 758, in show\n",
      "    _show_inputs_outputs(args.dir, args.tag_set, args.signature_def)\n",
      "  File \"C:\\Users\\schre\\anaconda3\\envs\\tf_dev\\lib\\site-packages\\tensorflow\\python\\tools\\saved_model_cli.py\", line 152, in _show_inputs_outputs\n",
      "    meta_graph_def = saved_model_utils.get_meta_graph_def(saved_model_dir,\n",
      "  File \"C:\\Users\\schre\\anaconda3\\envs\\tf_dev\\lib\\site-packages\\tensorflow\\python\\tools\\saved_model_utils.py\", line 113, in get_meta_graph_def\n",
      "    saved_model = read_saved_model(saved_model_dir)\n",
      "  File \"C:\\Users\\schre\\anaconda3\\envs\\tf_dev\\lib\\site-packages\\tensorflow\\python\\tools\\saved_model_utils.py\", line 51, in read_saved_model\n",
      "    raise IOError(\"SavedModel file does not exist at: %s\" % saved_model_dir)\n",
      "OSError: SavedModel file does not exist at: 'my_mnist_model\\0001'\n"
     ]
    }
   ],
   "source": [
    "!saved_model_cli show --dir '{model_path}' --tag_set serve \\\n",
    "                      --signature_def serving_default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For even more details, you can run the following command:\n",
    "\n",
    "```!saved_model_cli show --dir '{model_path}' --all```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing and Starting TensorFlow Serving\n",
    "If you are running this notebook in Colab or Kaggle, TensorFlow Server needs to be installed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"google.colab\" in sys.modules or \"kaggle_secrets\" in sys.modules:\n",
    "    url = \"https://storage.googleapis.com/tensorflow-serving-apt\"\n",
    "    src = \"stable tensorflow-model-server tensorflow-model-server-universal\"\n",
    "    !echo 'deb {url} {src}' > /etc/apt/sources.list.d/tensorflow-serving.list\n",
    "    !curl '{url}/tensorflow-serving.release.pub.gpg' | apt-key add -\n",
    "    !apt update -q && apt-get install -y tensorflow-model-server\n",
    "    %pip install -q -U tensorflow-serving-api"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If ```tensorflow_model_server``` is installed (e.g., if you are running this notebook in Colab), then the following 2 cells will start the server. If your OS is Windows, you may need to run the ```tensorflow_model_server``` command in a terminal, and replace ${MODEL_DIR} with the full path to the my_mnist_model directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"MODEL_DIR\"] = str(model_path.parent.absolute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash --bg\n",
    "tensorflow_model_server \\\n",
    "    --port=8500 \\\n",
    "    --rest_api_port=8501 \\\n",
    "    --model_name=my_mnist_model \\\n",
    "    --model_base_path=\"${MODEL_DIR}\" >my_server.log 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "time.sleep(2) # let's wait a couple seconds for the server to start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are running this notebook on your own machine, and you prefer to install TF Serving using Docker, first make sure Docker is installed, then run the following commands in a terminal. You must replace ```/path/to/my_mnist_model``` with the appropriate absolute path to the ```my_mnist_model``` directory, but do not modify the container path ```/models/my_mnist_model```.\n",
    "\n",
    "```\n",
    "docker pull tensorflow/serving  # downloads the latest TF Serving image\n",
    "\n",
    "docker run -it --rm -v \"/path/to/my_mnist_model:/models/my_mnist_model\" \\\n",
    "    -p 8500:8500 -p 8501:8501 -e MODEL_NAME=my_mnist_model tensorflow/serving\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying TF Serving through the REST API\n",
    "Next, let's send a REST query to TF Serving:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
